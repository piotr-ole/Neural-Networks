{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pandas as pd\n",
    "from mnist import MNIST #requires pip install python-mnist\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import winsound\n",
    "\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, hidden_layers, neurons_per_layer, data_folder, eta, lambd, mini_batch_size):\n",
    "        '''\n",
    "        Initializes the Network with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_layers: number of hidden layers in the Network\n",
    "        neuron_per_layer: number of neurons in each hidden layer\n",
    "        data_folder: path of the folder where MNIST data is stored\n",
    "        eta: learining rate\n",
    "        lambda: parameter of regularization\n",
    "        mini_batch_size: number of training examples in each mini batch\n",
    "        '''\n",
    "        self.k = hidden_layers + 2\n",
    "        self.n = neurons_per_layer # not including bias\n",
    "        self.data_folder = data_folder\n",
    "        self.X, self.labels = self.load_data('train')\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.weights , self.biases = self.set_up()\n",
    "        self.eta = eta\n",
    "        self.lambd = lambd\n",
    "        self.classify_table = np.zeros((10,10))\n",
    "        self.bad_classifications = list()\n",
    "    \n",
    "    def load_data(self, set_type, normalize = True):\n",
    "        '''\n",
    "        Loads data from MINST dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        set_type: specyfies which set should be loaded. 'train' for training, 'test' for testing\n",
    "        '''\n",
    "        data = MNIST(self.data_folder)\n",
    "        if (set_type == 'train'):\n",
    "            images, labels = data.load_training()\n",
    "        else:\n",
    "            images, labels = data.load_testing()\n",
    "        images, labels = np.transpose(np.array(images)), self.vector_label(labels)\n",
    "        if (normalize == True):\n",
    "            for i in range(0, np.size(images, 1)):\n",
    "                images[:, i] = (images[:, i] - np.mean(images[:, i])) / np.std(images[:, i])\n",
    "        return images, labels\n",
    "    \n",
    "    def vector_label(self, labels):\n",
    "        '''\n",
    "        Transforms each element of labels list into a vector (10x1) of zeros with 1 on the index equal to element value.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        labels: list of values\n",
    "        '''\n",
    "        lab = np.zeros((10, len(labels)))\n",
    "        lab[labels, range(0, len(labels))] = 1\n",
    "        return lab\n",
    "    \n",
    "    def classification_table(self):\n",
    "        '''\n",
    "        Returns classification table of testing examples. Correct class are coresponding with rows and columns\n",
    "        coresponds with network classification.\n",
    "        '''\n",
    "        return pd.DataFrame(self.classify_table)\n",
    "     \n",
    "    def set_up(self):\n",
    "        '''\n",
    "        Set up weights and biases with random values from normal distribution.\n",
    "        '''\n",
    "        w = list()\n",
    "        b = list()\n",
    "        w.append(np.random.randn(self.n, 784) / 10)  #s_j x (s_j + 1)\n",
    "        b.append(np.random.randn(self.n, 1) / 10)\n",
    "        for i in range(1, self.k - 2):\n",
    "            w.append(np.random.randn(self.n, self.n) / 10)\n",
    "            b.append(np.random.randn(self.n, 1) / 10)\n",
    "        w.append(np.random.randn(10, self.n) / 10)\n",
    "        b.append(np.random.randn(10, 1) / 10)\n",
    "        return w, b\n",
    "    \n",
    "    def activation(self, w , x, b):\n",
    "        '''\n",
    "        Calculates sigmoid function over equation (w @ x) + b. Where @ means matrix multiplication.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        w: matrix of weigths\n",
    "        x: matrix of inputs\n",
    "        b: vector of biases\n",
    "        '''\n",
    "        z = (w @ x) + b\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def derivative(self, coef, index):\n",
    "        '''\n",
    "        Calculates derivative od sigmoid function.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        coef: list of outputs from each layer\n",
    "        index: number of layer\n",
    "        '''\n",
    "        return (1 - coef[index]) * coef[index]\n",
    "    \n",
    "    def forwardpropagation(self, x):\n",
    "        '''\n",
    "        Calculates outputs from each layer of network using forward propagation algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: matrix of inputs, each column coresponds with one example\n",
    "        '''\n",
    "        a = list()\n",
    "        a.append(x)\n",
    "        for i in range(0, self.k - 1):\n",
    "            a.append(self.activation(self.weights[i], a[-1], self.biases[i]))\n",
    "        return a\n",
    "    \n",
    "    def backprop(self, a, y):\n",
    "        '''\n",
    "        Calculates errors in each layer of network using backpropagation algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        a: list of matrices with outputs from each layer, each column in each matrix coresponds with one\n",
    "           training/testing example\n",
    "        y: matrix of true classification for given examples\n",
    "        '''\n",
    "        delta = deque()\n",
    "        delta.appendleft((a[self.k - 1] - y))\n",
    "        for j in range(self.k - 2, 0, -1):\n",
    "            delta.appendleft((self.weights[j].T @ delta[0]) * self.derivative(a,j))\n",
    "        return list(delta)\n",
    "    \n",
    "    def backpropagation(self, mini_batch):\n",
    "        '''\n",
    "        Calculates forward and backpropagation for given mini batch.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        mini_batch: tuple (x, y), where x is matrix of examples and y matrix of labels.\n",
    "        '''\n",
    "        x, y = mini_batch\n",
    "        a = self.forwardpropagation(x)\n",
    "        delta = self.backprop(a, y)\n",
    "        return delta, a\n",
    "        \n",
    "    def gradient_descent(self, mini_batch):\n",
    "        '''\n",
    "        Calculates gradient over given mini batch and updates weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        mini_batch: tuple (x, y), where x is matrix of examples and y matrix of labels.\n",
    "        '''\n",
    "        deltas, ases = self.backpropagation(mini_batch)\n",
    "        for l in range(self.k - 2, -1 , -1):\n",
    "            self.weights[l] = self.weights[l] - ((self.eta / self.mini_batch_size) * (deltas[l] @ ases[l].T + \\\n",
    "                                                                           self.lambd * self.weights[l]))\n",
    "            self.biases[l] = self.biases[l] - ((self.eta / self.mini_batch_size) * np.sum(deltas[l], axis = 1).reshape(-1,1))\n",
    "    \n",
    "    def cost(self, x, lab):\n",
    "        '''\n",
    "        Calculates value of cost function.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: matrix of examples; each column of the matrix coresponds with one example\n",
    "        lab: matrix of labels; each column of the matrix coresponds with one example;\n",
    "        '''\n",
    "        results = self.forwardpropagation(x)\n",
    "        sum_squered_weights = [np.sum(w**2) for w in self.weights]\n",
    "        base = np.sum(lab * np.log(results[-1]) + (1 - lab) * np.log(1 - results[-1]))\n",
    "        return (-1 / np.size(lab, 1)) * (base - self.lambd * 0.5 * sum(sum_squered_weights))\n",
    "    \n",
    "    def shuffle(self):\n",
    "        '''\n",
    "        Shuffles training examples with their labels.\n",
    "        '''\n",
    "        perm = np.random.permutation(self.X.shape[1])\n",
    "        self.X = self.X[:, perm]\n",
    "        self.labels = self.labels[:, perm]\n",
    "\n",
    "    def visualise_effectiveness_by_class(self):\n",
    "        '''\n",
    "        Plots a barplot showing rate of correct classified examples for each class separately.\n",
    "        '''\n",
    "        index = np.arange(10)\n",
    "        results = [ self.classify_table[i,i] / np.sum(self.classify_table[i, :]) for i in range(0,10)]\n",
    "        plt.bar(index, results, color = ['#2ca25f', '#2c7fb8'])\n",
    "        plt.title(\"Rate of correct classifications to each class\")\n",
    "        plt.xlabel('Class')\n",
    "        plt.xticks(index)\n",
    "        plt.show()\n",
    "        \n",
    "    def visualise_errors_by_class(self):\n",
    "        '''\n",
    "        Plots a barplot with classes on x-axis and number of bad classifications in each class. Colors division represents\n",
    "        proportion showing to which class were classified examples that were classified incorrect.\n",
    "        '''\n",
    "        index = np.arange(10)\n",
    "        p = list()\n",
    "        table = copy.deepcopy(self.classify_table)\n",
    "        table[np.argmax(table, 0), np.argmax(table, 1)] = 0\n",
    "        p.append(plt.bar(index, table[:, 0]))\n",
    "        for i in range(1, 10):\n",
    "            p.append(plt.bar(index, table[:, i], bottom = np.sum(table[:, 0:i], 1)))\n",
    "        plt.title(\"Number of incorrect classifications\")\n",
    "        plt.xticks(index)\n",
    "        plt.xlabel(\"Correct class\")\n",
    "        plt.legend(list(range(0,10)), title = \"class returned by network\", bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        \n",
    "    def visualise_errors_for_class(self, cl):\n",
    "        '''\n",
    "        Plots number of incorrect classifications of elements from specified class with division where elements were\n",
    "        classified by network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cl: class number\n",
    "        '''\n",
    "        index = np.arange(10)\n",
    "        p = list()\n",
    "        table = copy.deepcopy(self.classify_table)\n",
    "        table[np.argmax(table, 0), np.argmax(table, 1)] = 0\n",
    "        plt.bar(index, table[:, cl])\n",
    "        plt.xticks(index)\n",
    "        plt.title(\"Number of incorrect classifications of elements from class {}\".format(cl))\n",
    "        plt.xlabel(\"Class returned by network\")\n",
    "        plt.show()\n",
    "        \n",
    "    def visualise_incorrect_images(self, indexes):\n",
    "        '''\n",
    "        Plots specified images of incorrect classified images.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        indexes: list of indexes to self.bad_classifications list object which is storing incorrectly classified examples.\n",
    "        '''\n",
    "        images, labels = self.load_data('test', False)\n",
    "        for (i, index) in enumerate(indexes):\n",
    "            plt.subplot(1 , len(indexes) , i+1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(images[:,self.bad_classifications[index][0]].reshape(28,28))\n",
    "        plt.show()\n",
    "            \n",
    "    \n",
    "    def visualise_all_incorrect_images(self):\n",
    "        '''\n",
    "        Shows all incorrectly classified pictures from testing set.\n",
    "        '''\n",
    "        plt.figure(figsize=(20, 200), dpi= 80)\n",
    "        images, labels = self.load_data('test', False)\n",
    "        cols = 5\n",
    "        rows = math.floor(len(self.bad_classifications) / cols) + 1\n",
    "        for i in range(1, rows + 1):\n",
    "            for j in range(1, cols + 1):\n",
    "                if (len(self.bad_classifications) > cols*(i-1) + (j-1)):\n",
    "                    plt.subplot(rows, cols, (i - 1) * cols + j)\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.imshow(images[:,self.bad_classifications[cols*(i-1) + (j-1)][0]].reshape(28,28))\n",
    "        plt.show()\n",
    "            \n",
    "    def roc_analysis(self, xlim, ylim):\n",
    "        '''\n",
    "        Plots ROC curve with AUC values calculated for elements from testing set with class division.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        xlim, ylim: lists of two parameters specifing start and end of respectively x and y axis\n",
    "        '''\n",
    "        images , labels = self.load_data('test')\n",
    "        results = self.forwardpropagation(images)[-1]\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(0, 10):\n",
    "            fpr[i], tpr[i], _ = roc_curve(labels[i, :], results[i, :])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.figure(figsize=(7, 5), dpi= 80)\n",
    "        for i in range(0, 10):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve (AUC = {:0.5f}) for {} class'.format(roc_auc[i], i))\n",
    "        plt.xlim(xlim)\n",
    "        plt.ylim(ylim)\n",
    "        plt.xlabel('False Positive Rate', size = 12)\n",
    "        plt.ylabel('True Positive Rate', size = 12)\n",
    "        plt.title('ROC curve for binary classification task', size = 13)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def train(self, epochs):\n",
    "        '''\n",
    "        Trains the network. Plots the cost function value in each epoch of training for training and testing set.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        epochs: number of epochs to train\n",
    "        '''\n",
    "        images, labels = self.load_data('test')\n",
    "        for i in range(0, epochs): \n",
    "            self.shuffle()\n",
    "            mini_batches = [(self.X[:,i:i+self.mini_batch_size], self.labels[:,i:i+self.mini_batch_size]) \\\n",
    "                                for i in range(0, np.size(self.labels, 1), self.mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.gradient_descent(mini_batch)\n",
    "            plt.plot(i, self.cost(self.X, self.labels), 'bo')\n",
    "            plt.plot(i, self.cost(images, labels), 'yo')\n",
    "        plt.title(\"Value of the cost function\")\n",
    "        plt.legend(['train', 'test'])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.show()\n",
    "        winsound.Beep(1500, 1000)\n",
    "                  \n",
    "    def test(self):\n",
    "        '''\n",
    "        Creates cross-table of classifications in training set.\n",
    "        Returns rate of correctly classified examples in that set.\n",
    "        '''\n",
    "        self.classify_table = np.zeros((10,10))\n",
    "        images, labels = self.load_data('test')\n",
    "        result = self.forwardpropagation(images)[-1]\n",
    "        classify = np.argmax(result, 0)\n",
    "        good_label =  np.argmax(labels, 0)\n",
    "        i = 0\n",
    "        for lab, cl in zip(good_label, classify):\n",
    "            self.classify_table[lab, cl] += 1\n",
    "            if (lab != cl):\n",
    "                self.bad_classifications.append((i, cl, lab))\n",
    "            i = i + 1\n",
    "        s = np.sum(classify == good_label)\n",
    "        return s / np.size(labels, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
