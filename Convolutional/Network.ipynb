{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Piotr Olesiejuk kod programu.ipynb","version":"0.3.2","provenance":[{"file_id":"1zmRAivFcAfaD_vGDA0qK3qIU56_MrLNB","timestamp":1555370436259},{"file_id":"17FAQuo2_-bkhH9SNHnL7_PPz3Zr72xnU","timestamp":1555237225603},{"file_id":"1eA0XLwfcG1O-3rHBALGh34D5klUXAn_o","timestamp":1555232720841},{"file_id":"1XcnucCCmXmBrYnrQuaX8-hF2uCeu9lpU","timestamp":1555066687450}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ALPvpXFPsKnL","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class ConvUnitModule(nn.Module):\n","    \n","    def __init__(self, input_channels, output_channels, kernel_size = 3, stride = 1, padding = 0):\n","        super(ConvUnitModule, self).__init__()\n","        self.conv = nn.Conv2d(in_channels = input_channels, out_channels = output_channels,\n","                              stride = stride, kernel_size = kernel_size, padding = padding)\n","        self.relu = nn.ReLU()\n","        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        return x\n","    \n","class FcUnitModule(nn.Module):\n","    \n","    def __init__(self, input_size, output_size):\n","        super(FcUnitModule, self).__init__()\n","        self.linear = nn.Linear(input_size, output_size)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = self.linear(x)\n","        x = self.relu(x)\n","        return x\n","\n","class LeNetModel(nn.Module):\n","        \n","    def __init__(self, number_of_classes = 10):\n","        super(LeNetModel, self).__init__()\n","        # Architecture\n","        \n","        self.unit1 = ConvUnitModule(3, 6, 5)\n","        self.unit2 = ConvUnitModule(6, 16, 5)\n","        self.unit3 = FcUnitModule(400, 120) # 5*16*5\n","        self.unit4 = FcUnitModule(120,84)\n","        self.unit5 = nn.Linear(84, number_of_classes)\n","        self.conv_network = nn.Sequential(self.unit1, self.unit2)\n","        self.fc_network = nn.Sequential(self.unit3, self.unit4)\n","            \n","\n","    def forward(self, x):\n","        x = self.conv_network(x)\n","        x = x.view(-1, x.shape[0] , 400)\n","        x = self.fc_network(x)\n","        x = self.unit5(x)\n","        return x\n","\n","class SecondModel(nn.Module):\n","    def __init__(self, number_of_classes = 10):\n","        super(SecondModel, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, stride = 1, kernel_size = 3, padding = 1)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, stride = 1, kernel_size = 3, padding = 1)\n","        self.relu2 = nn.ReLU()\n","        self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, stride = 1, kernel_size = 3, padding = 0)\n","        self.relu3 = nn.ReLU()\n","        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.linear1 = nn.Linear(7 * 7 * 32, 300)\n","        self.relu4 = nn.ReLU()\n","        self.drop1 = nn.Dropout(p = 0.5)\n","        self.linear2 = nn.Linear(300, 10)\n","        \n","        self.net = nn.Sequential(self.conv1, self.relu1, self.conv2, self.relu2,\n","                                 self.maxpool1, self.conv3, self.relu3, self.maxpool2)\n","                                             \n","    def forward(self, x):\n","        x = self.net(x)\n","        x = x.view(-1, x.shape[0] , 7 * 7 * 32)\n","        x = self.linear1(x)\n","        x = self.relu4(x)\n","        x = self.drop1(x)\n","        x = self.linear2(x)\n","        return x\n","      \n","class ThirdModel(nn.Module):\n","    def __init__(self, number_of_classes = 10):\n","        super(ThirdModel, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, stride = 1, kernel_size = 3, padding = 1)\n","        self.batchnorm1 = nn.BatchNorm2d(num_features = 16)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, stride = 1, kernel_size = 3, padding = 1)\n","        self.batchnorm2 = nn.BatchNorm2d(num_features = 32)\n","        self.relu2 = nn.ReLU()\n","        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64, stride = 1, kernel_size = 3, padding = 0)\n","        self.batchnorm3 = nn.BatchNorm2d(num_features = 64)\n","        self.relu3 = nn.ReLU()\n","        self.maxpool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.linear1 = nn.Linear(7 * 7 * 64, 300)\n","        self.relu4 = nn.ReLU()\n","        self.drop1 = nn.Dropout(p = 0.5)\n","        self.linear2 = nn.Linear(300, 10)\n","        \n","        self.net = nn.Sequential(self.conv1, self.batchnorm1, self.relu1, self.conv2, self.batchnorm2, self.relu2,\n","                                 self.maxpool2, self.conv3, self.batchnorm3, self.relu3, self.maxpool3)\n","                                             \n","    def forward(self, x):\n","        x = self.net(x)\n","        x = x.view(-1, x.shape[0] , 7 * 7 * 64)\n","        x = self.linear1(x)\n","        x = self.relu4(x)\n","        x = self.drop1(x)\n","        x = self.linear2(x)\n","        return x\n","  \n","\n","\n","class FourthModel(nn.Module):\n","    def __init__(self, number_of_classes = 10):\n","        super(FourthModel, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, stride = 1, kernel_size = 3, padding = 1)\n","        self.batchnorm1 = nn.BatchNorm2d(num_features = 16)\n","        self.relu1 = nn.ReLU()\n","        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, stride = 1, kernel_size = 3, padding = 1)\n","        self.batchnorm2 = nn.BatchNorm2d(num_features = 32)\n","        self.relu2 = nn.ReLU()\n","        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 32, stride = 1, kernel_size = 3, padding = 1)\n","        self.batchnorm3 = nn.BatchNorm2d(num_features = 32)\n","        self.relu3 = nn.ReLU()\n","        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 32, stride = 1, kernel_size = 3, padding = 1)\n","        self.batchnorm4 = nn.BatchNorm2d(num_features = 32)\n","        self.relu4 = nn.ReLU()\n","        self.maxpool4 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.conv5 = nn.Conv2d(in_channels = 32, out_channels = 64, stride = 1, kernel_size = 3, padding = 1)\n","        self.batchnorm5 = nn.BatchNorm2d(num_features = 64)\n","        self.relu5 = nn.ReLU()\n","        self.conv6 = nn.Conv2d(in_channels = 64, out_channels = 128, stride = 1, kernel_size = 3, padding = 0)\n","        self.batchnorm6 = nn.BatchNorm2d(num_features = 128)\n","        self.relu6 = nn.ReLU()\n","        self.maxpool6 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        self.linear1 = nn.Linear(3 * 3 * 128, 1300)\n","        self.relu21 = nn.ReLU()\n","        self.drop1 = nn.Dropout(p = 0.5)\n","        self.linear2 = nn.Linear(1300, 300)\n","        self.relu22 = nn.ReLU()\n","        self.drop2 = nn.Dropout(p = 0.5)\n","        self.linear3 = nn.Linear(300, 10)\n","        \n","        self.conv_net = nn.Sequential(self.conv1, self.batchnorm1, self.relu1, self.conv2, self.batchnorm2, self.relu2, self.maxpool2,\n","                                self.conv3, self.batchnorm3, self.relu3, self.conv4, self.batchnorm4, self.relu4, self.maxpool4,\n","                                self.conv5, self.batchnorm5, self.relu5, self.conv6, self.batchnorm6, self.relu6, self.maxpool6)\n","        \n","        self.linear_net = nn.Sequential(self.linear1, self.relu21, self.drop1, self.linear2, self.relu22, self.drop2, self.linear3)\n","                                             \n","    def forward(self, x):\n","        x = self.conv_net(x)\n","        x = x.view(-1, x.shape[0] , 3 * 3 * 128)\n","        x = self.linear_net(x)\n","        return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rlYJTQbttf22","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import torch\n","\n","def save_network_state(model, optimizer, epoch, loss, lr, batch_size, dataset_name, training_track, breaking_points, l2_reg, model_number):\n","        optim_name = str(optimizer).split(' ')[0]\n","        model_name = \"{}_{}_epoch_{}_lr_{:8.7f}_batch_{}_loss_{:4.3f}.tar\".format(dataset_name, optim_name, epoch, lr, batch_size, loss)\n","        path = create_dirs_tree(model_number)\n","        path = path + model_name\n","        torch.save({'Optimizer': optim_name,\n","                    'Starting learning rate': lr,\n","                    'epoch': epoch,\n","                    'loss': loss,\n","                    'training_track': training_track,\n","                    'l2_reg': l2_reg,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'breaking_points': breaking_points\n","                   }, path)\n","        return path\n","\n","        \n","def create_dirs_tree(model_num):\n","    folder_name = \"Model {}\".format(model_num)\n","    dirs = \"/content/gdrive/My Drive/Project_conv/Models/Good models/{}/\".format(folder_name)\n","    if not os.path.exists(dirs):\n","        os.makedirs(dirs)\n","    #    print(\"Directory \" , dirs ,  \" Created \")\n","    #else:    \n","    #    print(\"Directory \" , dirs ,  \" already exists\")\n","    return dirs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KEzxybTBtkO9","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import copy\n","from sklearn import svm, datasets\n","from sklearn.metrics import roc_curve, auc\n","\n","\n","def visualise_learining_process(epochs, losses_on_test, losses_on_train, xtick, breaking_points):\n","    plt.plot(epochs, losses_on_test, 'yo')\n","    plt.plot(epochs, losses_on_train, 'bo')\n","    for point in breaking_points:\n","      plt.axvline(x= point, color = 'red', linestyle = '--')\n","    plt.title(\"Value of the cost function\")\n","    plt.legend(['test', 'train'])\n","    plt.xlabel(\"Epoch\")\n","    plt.xticks(np.arange(0, len(epochs), xtick))\n","    plt.show()\n","\n","\n","def visualise_errors_for_class(classify_table, class_index):\n","    '''\n","    Plots number of incorrect classifications of elements from specified class with division where elements were\n","    classified by network.\n","\n","    Parameters:\n","    -----------\n","    class_index: class number\n","    '''\n","    indices = np.arange(10)\n","    p = list()\n","    table = copy.deepcopy(classify_table)\n","    table[np.argmax(table, 0), np.argmax(table, 1)] = 0\n","    plt.bar(indices, table[:, class_index])\n","    plt.xticks(indices)\n","    plt.title(\"Number of incorrect classifications of elements from class {}\".format(class_index))\n","    plt.xlabel(\"Class returned by network\")\n","    plt.show()\n","    \n","def visualise_errors_by_class(classify_table):\n","    '''\n","    Plots a barplot with classes on x-axis and number of bad classifications in each class. Colors division represents\n","    proportion showing to which class were classified examples that were classified incorrect.\n","    '''\n","    indices = np.arange(10)\n","    p = list()\n","    table = copy.deepcopy(classify_table)\n","    table[np.argmax(table, 0), np.argmax(table, 1)] = 0\n","    p.append(plt.bar(indices, table[:, 0]))\n","    for i in range(1, 10):\n","        p.append(plt.bar(indices, table[:, i], bottom = np.sum(table[:, 0:i], 1)))\n","    plt.title(\"Number of incorrect classifications\")\n","    plt.xticks(indices)\n","    plt.xlabel(\"Correct class\")\n","    plt.legend(list(range(0,10)), title = \"class returned by network\", bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n","    plt.show()\n","    \n","def visualise_effectiveness_by_class(classify_table):\n","    '''\n","    Plots a barplot showing rate of correct classified examples for each class separately.\n","    '''\n","    indices = np.arange(10)\n","    results = [ classify_table[i,i] / np.sum(classify_table[i, :]) for i in range(10)]\n","    plt.bar(indices, results, color = ['#2ca25f', '#2c7fb8'])\n","    plt.title(\"Rate of correct classifications to each class\")\n","    plt.xlabel('Class')\n","    plt.xticks(indices)\n","    plt.show()\n","    \n","def roc_analysis(labels, results, xlim, ylim, draw_diag = False):\n","    '''\n","    Plots ROC curve with AUC values calculated for elements from testing set with class division.\n","\n","    Parameters:\n","    -----------\n","    xlim, ylim: lists of two parameters specifing start and end of respectively x and y axis\n","    '''\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(0, 10):\n","        fpr[i], tpr[i], _ = roc_curve(labels[i, :], results[i, :])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","    plt.figure(figsize=(7, 5), dpi= 80)\n","    for i in range(0, 10):\n","        plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve (AUC = {:0.5f}) for {} class'.format(roc_auc[i], i))\n","    if (draw_diag == True):\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim(xlim)\n","    plt.ylim(ylim)\n","    plt.xlabel('False Positive Rate', size = 12)\n","    plt.ylabel('True Positive Rate', size = 12)\n","    plt.title('ROC curve for binary classification task', size = 13)\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n","    plt.show()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2CVOMiHQtoWm","colab_type":"code","colab":{}},"cell_type":"code","source":["# torch libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import numpy as np\n","from torchvision.datasets import CIFAR10\n","from torchvision.transforms import transforms\n","from torch.utils.data import DataLoader\n","from torch.optim import Adadelta, Adam\n","\n","# Others\n","import matplotlib.pyplot as plt\n","from IPython.core.debugger import set_trace\n","\n","\n","# Network\n","class Network:\n","    \n","    def __init__(self, batch_size, learning_rate, l2_reg, model_number):\n","        self.model_number = model_number\n","        self.batch_size = batch_size\n","        self.lr = learning_rate\n","        self.l2_reg = l2_reg\n","        self.train_trans , self.test_trans = self.transformations()\n","        self.train_loader, self.test_loader = self.data_loaders(self.batch_size)\n","        self.model = FourthModel(number_of_classes = 10).cuda()\n","        self.optimizer = Adam(self.model.parameters(), lr = self.lr, weight_decay=self.l2_reg)\n","        self.loss_fun = nn.CrossEntropyLoss()\n","        self.smallest_loss = self.test_evaluate()[1]\n","        self.starting_epoch = 1\n","        self.classify_table = np.zeros((10,10))\n","        self.training_tracking = [list(), list()]\n","        self.last_model_path = ''\n","        self.breaking_points = []\n","        self.initialize_logs()\n","        self.model_loading_control = False\n","        \n","    def transformations(self):\n","        _mean = [0.5, 0.5, 0.5]\n","        _std =  [0.5, 0.5, 0.5]\n","        train_trans = transforms.Compose([transforms.RandomCrop(32), transforms.RandomHorizontalFlip(), transforms.RandomRotation(15), \n","                                          transforms.ColorJitter(0.2, 0.2, 0.2), transforms.ToTensor(), transforms.Normalize(_mean, _std)])\n","        test_trans = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(_mean, _std)])\n","        return train_trans, test_trans\n","    \n","    def data_loaders(self, batch_train_size, batch_test_size = 10000): ###???\n","        train_set, test_set = self.datasets()\n","        train_loader = DataLoader(train_set, batch_size = batch_train_size, shuffle = True, num_workers = 4)\n","        test_loader = DataLoader(test_set, batch_size = batch_test_size, shuffle = False, num_workers = 4)\n","        return train_loader, test_loader\n","    \n","    def datasets(self):\n","        train_set = CIFAR10(root = \"./data\", train = True, transform = self.train_trans, download = True)\n","        test_set = CIFAR10(root = \"./data\", train = False, transform = self.test_trans, download = True)\n","        return train_set, test_set\n","\n","    def train(self, epochs):\n","        #entering training mode\n","        print(\"Starting loss: {}\".format(self.smallest_loss))\n","        epoch = self.starting_epoch\n","        epochs = epoch + epochs - 1\n","        cuda0 = torch.device('cuda:0')\n","        while epoch <= epochs:\n","            for i, (img, label) in enumerate(self.train_loader):\n","                img = img.to(cuda0)\n","                label = label.to(cuda0)\n","                self.model.train()\n","                self.optimizer.zero_grad()\n","                prediction = self.model(img)\n","                loss = self.loss_fun(prediction[0], label)\n","                loss.backward()\n","                self.optimizer.step()\n","            epoch = self.epoch_model_evaluation(epoch, epochs)\n","            if (self.lr < 1e-7):\n","              self.end_logs('Learning rate below 1e-7')\n","              break\n","        self.visualise_learing_process()\n","        self.starting_epoch = epochs + 1\n","        \n","    def epoch_model_evaluation(self, epoch, epochs):\n","        with torch.no_grad():\n","            test_eval = self.test_evaluate()\n","            train_eval = self.train_evaluate()\n","            self.training_tracking[0].append(float(test_eval[1]))\n","            self.training_tracking[1].append(float(train_eval[1]))\n","        if (test_eval[1] < self.smallest_loss):\n","            self.smallest_loss = test_eval[1]\n","            self.model_loading_control = False\n","            path = self.save_model(epoch, self.smallest_loss)\n","            self.update_logs(epoch, epochs, test_eval[1], test_eval[0], train_eval[1], train_eval[0], True, False)\n","            return epoch + 1\n","        elif ((test_eval[1] > 1.05 * self.smallest_loss) or sum([self.smallest_loss < self.training_tracking[0][i] for i in range(-1, -3, -1)]) == 2):\n","            if (self.model_loading_control == True):\n","              self.end_logs('Same models loaded in a row.')\n","              return epochs + 1\n","            self.change_optimizer(self.last_model_path)\n","            self.model_loading_control = True\n","            self.breaking_points.append(self.starting_epoch)\n","            self.update_logs(epoch, epochs, test_eval[1], test_eval[0], train_eval[1], train_eval[0], False, True)\n","            return self.starting_epoch\n","        else:\n","            self.update_logs(epoch, epochs, test_eval[1], test_eval[0], train_eval[1], train_eval[0], False, False)\n","            return epoch + 1\n","          \n","    def end_logs(self, message):\n","        text = 'Training ended. {}'.format(message)\n","        print(text)\n","        with open('/content/gdrive/My Drive/Project_conv/Models/Good models/model{}.txt'.format(self.model_number), 'a') as f:\n","          f.write(text)\n","\n","    def update_logs(self, epoch, epochs, test_loss, test_acc, train_loss, train_acc, is_saving, is_model_changed):\n","        text = \"Epoch {}/{} loss on test: {:6.5f}, acc on test: {:5.4f}, loss on train: {:6.5f}, acc on train: {:5.4f}\".format(epoch, epochs, test_loss, test_acc, train_loss, train_acc)\n","        if (is_saving == True and is_model_changed == False):\n","          text += \", model saved.\"\n","        elif (is_saving == False and is_model_changed == True):\n","          text = 'Model after epoch {} loaded, now training with learning rate = {}'.format(self.starting_epoch - 1, self.lr)\n","        print(text)\n","        text += '\\n'\n","        with open('/content/gdrive/My Drive/Project_conv/Models/Good models/model{}.txt'.format(self.model_number, self.model_number), 'a') as f:\n","          f.write(text)\n","    \n","    def initialize_logs(self):\n","        text = 'MODEL: {}\\n\\n{}\\n\\nMODEL PARAMETERS:\\n\\nStarting learning rate: {}\\nRegularization l2 weight: {}\\nBatch size: {}\\nOptimizer {}\\n\\n'.format(\n","            str(self.model).split('(')[0],str(self.model), self.lr, self.l2_reg, self.batch_size, str(self.optimizer).split(' ')[0])\n","        trans = 'TRAIN SET TRANSFORMATIONS:\\n\\n{}\\n\\nTEST SET TRANSFORMATIONS:\\n\\n{}\\n\\nTRAINING PROCESS:\\n\\n'.format(\n","                    str(self.train_trans), str(self.test_trans))\n","        with open('/content/gdrive/My Drive/Project_conv/Models/Good models/model{}.txt'.format(self.model_number), 'w') as f:\n","          f.write(text)\n","          f.write(trans)\n","        \n","    def save_model(self, epoch, loss):\n","        path = save_network_state(self.model, self.optimizer, epoch, loss, self.lr,\n","                                           self.batch_size, 'Cifar10', self.training_tracking,\n","                                           self.breaking_points, self.l2_reg, self.model_number)\n","        self.last_model_path = path\n","        return path\n","           \n","    def load_model(self, path, train = False):\n","        state_data = torch.load(path)\n","        self.model.load_state_dict(state_data['model_state_dict'])\n","        self.optimizer.load_state_dict(state_data['optimizer_state_dict'])\n","        self.starting_epoch = state_data['epoch'] + 1\n","        self.smallest_loss = state_data['loss']\n","        self.training_tracking = state_data['training_track']\n","        self.lr = state_data['Starting learning rate']\n","        self.l2_reg = state_data['l2_reg']\n","        self.last_model_path = path\n","        self.breaking_points = state_data['breaking_points']\n","        print('Model loaded:\\nOptimizer: {}\\nStarting learning rate: {}\\nEpoch: {}\\nLoss: {}\\n'.format( \\\n","            state_data['Optimizer'], state_data['Starting learning rate'], state_data['epoch'], state_data['loss']))\n","        if (train == True):\n","            self.model.train()\n","        else:\n","            self.model.eval()\n","        \n","    def train_evaluate(self):\n","        with torch.no_grad():\n","            self.model.eval()\n","            class_table = np.zeros((10,10))\n","            loss = 0.0\n","            true_counter = 0\n","            cuda0 = torch.device('cuda:0')\n","            for i, (img, label) in enumerate(self.train_loader):\n","                img = img.to(cuda0)\n","                label = label.to(cuda0)\n","                prediction = self.model(img)\n","                loss += self.loss_fun(prediction[0], label)\n","                _ , prediction = torch.max(prediction[0].data, 1)\n","                self.update_classify_table(class_table, prediction, label.data)\n","                true_counter += torch.sum(prediction == label.data)\n","            return (true_counter.cpu().numpy() / 50000) , (loss * self.batch_size / 50000)\n","\n","    def test_evaluate(self):\n","        with torch.no_grad():\n","            self.model.eval()\n","            self.classify_table = np.zeros((10,10))\n","            true_counter = 0\n","            loss = 0.0\n","            cuda0 = torch.device('cuda:0')\n","            for i, (img, label) in enumerate(self.test_loader):\n","                img = img.to(cuda0)\n","                label = label.to(cuda0)\n","                prediction = self.model(img)\n","                loss += self.loss_fun(prediction[0], label)\n","                _ , prediction = torch.max(prediction[0].data, 1)\n","                self.update_classify_table(self.classify_table, prediction, label.data)\n","                true_counter += torch.sum(prediction == label.data)\n","            return (true_counter.cpu().numpy() / 10000) , loss\n","    \n","    def change_optimizer(self, model_path): # nie jest gotowe\n","        self.load_model(model_path)\n","        self.lr = self.lr / 3\n","        for param_group in self.optimizer.param_groups:\n","            param_group[\"lr\"] = self.lr\n","    \n","    def load_model_with_hyperparameters(self, model_path,  learning_rate, l2_regularization):\n","        self.load_model(model_path)\n","        self.lr = learing_rate\n","        self.l2_reg = l2_regularization\n","        for param_group in self.optimizer.param_groups:\n","            param_group[\"lr\"] = self.lr\n","            param_group[\"weight_decay\"] = self.l2_reg\n","    \n","    def update_classify_table(self, classify_table, predictions, labels):\n","        for lab, pred in zip(labels, predictions):\n","            classify_table[lab, pred] += 1\n","    \n","    def vector_label(self, labels):\n","        lab = np.zeros((10, len(labels)))\n","        lab[labels, range(len(labels))] = 1\n","        return lab\n","            \n","    #visualisation\n","    def visualise_learing_process(self):\n","        epochs = range(1, len(self.training_tracking[0]) + 1)\n","        visualise_learining_process(epochs, self.training_tracking[0], self.training_tracking[1], 3, self.breaking_points)\n","    \n","    def visualise_errors_for_class(self, cl):\n","        visualise_errors_for_class(self.classify_table, cl)\n","    \n","    def visualise_effectiveness_by_class(self):\n","        visualise_effectiveness_by_class(self.classify_table)\n","        \n","    def visualise_errors_by_class(self):\n","        visualise_errors_by_class(self.classify_table)\n","    \n","    def roc_analysis(self, xlim = [0, 1], ylim = [0, 1], draw_diag = False): # Zastanowic sie czy nie da sie ladniej\n","        cuda0 = torch.device('cuda:0')\n","        _ , test_loader = self.data_loaders(self.batch_size, 10_000)\n","        self.model.eval()\n","        batch = list(enumerate(test_loader))\n","        images, labels = batch[0][1][0], batch[0][1][1]\n","        images = images.to(cuda0)\n","        labels = labels.to(cuda0)\n","        prediction = self.model(images)\n","        results = F.log_softmax(prediction[0], dim = 1)\n","        labels = labels.cpu().numpy()\n","        labels = self.vector_label(labels)\n","        roc_analysis(labels, results.detach().cpu().numpy().T, xlim, ylim, draw_diag)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bgHL_sU9H_Sb","colab_type":"code","outputId":"23b75589-81f5-4a2e-db61-9e5f096ea66b","executionInfo":{"status":"ok","timestamp":1555241481059,"user_tz":-120,"elapsed":6893,"user":{"displayName":"Piotr Olesiejuk","photoUrl":"","userId":"09352214183246267545"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["net = Network(100, 0.001, 0, 'test')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"metadata":{"id":"2t-zBoBRosWr","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]}]}